# -*- coding: utf-8 -*-
"""Cyber Crime

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VMO7mQNRrGo91LXmpS0YQwbO-rGqXfrO
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns #visualization
import matplotlib.pyplot as plt #visualization
# %matplotlib inline
import plotly.express as px
import plotly.graph_objects as go
import plotly.offline as py
import plotly.express as px

import os
import io

from google.colab import files
uploaded = files.upload()

df = pd.read_csv(io.BytesIO(uploaded['datafiles.csv']))

df.head()

df.shape

df.columns

df.describe()

df.info()

df.isnull().sum()

"""**Visualization**"""

corr = df.corr()
corr.style.background_gradient(cmap = 'coolwarm')

df.plot(kind='bar',x='State/UT',y='2019')
plt.show()

df.plot(kind='bar',x='State/UT',y='2020',color='r')
plt.show()

df.plot(kind='bar',x='State/UT',y='2021',color='g')
plt.show()

df.plot()

from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import Normalizer
from sklearn.model_selection import train_test_split


from scipy.stats import skew

# categorical features
categorical_feat = [feature for feature in df.columns if df[feature].dtypes=='O']
print('Total categorical features: ', len(categorical_feat))
print('\n',categorical_feat)

for c in df.columns:
    if df[c].dtype=='float16' or  df[c].dtype=='float32' or  df[c].dtype=='float64':
        df[c].fillna(df[c].mean())

#fill in -999 for categoricals
df = df.fillna(-999)
# Label Encoding
for f in df.columns:
    if df[f].dtype=='object': 
        lbl = LabelEncoder()
        lbl.fit(list(df[f].values))
        df[f] = lbl.transform(list(df[f].values))

from sklearn.model_selection import train_test_split
# Hot-Encode Categorical features
df = pd.get_dummies(df) 

# Splitting dataset back into X and test data
X = df[:len(df)]
test = df[len(df):]

X.shape

df.columns.tolist()

pip install --upgrade category_encoders

from category_encoders import OneHotEncoder
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler

cols_selected = ['Rate of Total Cyber Crimes (2021)++']
ohe = OneHotEncoder(cols=cols_selected, use_cat_names=True)
df_t = ohe.fit_transform(df[cols_selected+['2021']])

#scaler = MaxAbsScaler()
X = df_t.iloc[:,:-1]
y = df_t.iloc[:, -1].fillna(df_t.iloc[:, -1].mean()) / df_t.iloc[:, -1].max()

mdl = Ridge(alpha=0.1)
mdl.fit(X,y)

pd.Series(mdl.coef_, index=X.columns).sort_values().head(10).plot.barh()

ax = df.groupby('Rate of Total Cyber Crimes (2021)++')['2021'].mean().plot(kind='barh', figsize=(12,8),
title='Mean estimated Rate of Cyber Crimes 2021')
plt.xlabel('Mean estimated Cyber Crimes 2021 ++')
plt.ylabel('2021')
plt.show()

ax = df.groupby('Rate of Total Cyber Crimes (2021)++')['2020'].min().sort_values(ascending=True).plot(kind='barh', figsize=(12,8), color='r',
title='Mean estimated Rate of Cyber Crimes 2020')
plt.xlabel('Mean estimated Cyber Crimes')
plt.ylabel('2020')
plt.show()

ax = df.groupby('Rate of Total Cyber Crimes (2021)++')['2019'].max().sort_values(ascending=True).plot(kind='barh', figsize=(12,8), color='purple',
title='Max. Percentage Share of State/UT')
plt.xlabel('Max. Percentage Share of State/UT 2021')
plt.ylabel('2019')
plt.show()

ax = df.groupby('Rate of Total Cyber Crimes (2021)++')['2020','2021'].sum().plot(kind='bar', rot=45, figsize=(12,6),logy=True,
title='Rate of Total Cyber Crimes')
plt.xlabel('Rate of Total Cyber Crimes')
plt.ylabel('2021 ++')
plt.show()

ax = df.groupby('Mid-Year Projected Population (in Lakhs) (2021)+')['2019','2020'].sum().plot(kind='bar', rot=45, figsize=(12,6), logy=True,
title='Mid-Year Projected Population, in Lakhs')
plt.xlabel('Mid-Year Projected Population (in Lakhs) 2021 +')
plt.ylabel('2021++')
plt.show()

ax = df.groupby('2021')['Rate of Total Cyber Crimes (2021)++', 'Mid-Year Projected Population (in Lakhs) (2021)+'].sum().plot(kind='bar', rot=45, figsize=(12,6), logy=True,
                                                                 title='Rate of Cyber Crimes 2018')
plt.xlabel('2021')
plt.ylabel('Rate of Cyber Crimes & Mid-Year Projected Population')

plt.show()

import matplotlib.ticker as ticker
ax = sns.distplot(df['Rate of Total Cyber Crimes (2021)++'])
plt.xticks(rotation=45)
ax.xaxis.set_major_locator(ticker.MultipleLocator(2))
figsize=(10, 4)

plt.style.use('fivethirtyeight')
df.plot(subplots=True, figsize=(4, 4), sharex=False, sharey=False)
plt.show()

df = df.rename(columns={'State/UT':'state'})

fig, ax = plt.subplots(1,3, figsize = (20,6), sharex=True)
sns.countplot(x='Rate of Total Cyber Crimes (2021)++',data=df, palette="copper", ax=ax[0])

ax[0].title.set_text('Cyber Crimes')
plt.xticks(rotation=30)
plt.show()

"""Network Security **bold text**"""

from google.colab import files
uploaded = files.upload()

import json
from urllib.request import urlopen
import pandas as pd
import matplotlib.pyplot as plt
import io

df = pd.read_csv(io.BytesIO(uploaded['monoxor.csv']))

df.head()

df.describe(include = 'all')

df_new = df[['req/body/note/title', 'req/body/note/desc', 'isSafe']]
df_new.head()

mylabels='Safe','Unsafe'
cmap = plt.get_cmap('Spectral')
colors = [cmap(i) for i in np.linspace(0, 1, 8)]

plt.figure(figsize=(16,8))

plt.title("Safe and Unsafe Requests Distribution", size = 20)
plt.pie(df_new['isSafe'].value_counts(), labels=mylabels, autopct='%1.1f%%', shadow=True, colors=colors)
plt.show()

not_safe = df_new['isSafe'].isin([False])
df_not_safe = df_new[not_safe].reset_index(drop=True)

df_not_safe = pd.DataFrame(df_not_safe)
df_not_safe

safe = df_new['isSafe'].isin([True])
df_safe = df_new[safe].reset_index(drop=True)

df_safe

"""Maliciuos Words in Safe and Unsafe requests. """

import string
string.punctuation
def remove_punctuation(text):
    no_punct = [word for word in text if not word.isalpha()]
    no_punct = [word for word in no_punct if word!= '.']
    words_wo_punct = ''.join(no_punct)
   
    return words_wo_punct
mal_words_ns = df_not_safe['req/body/note/desc'].apply(lambda x: remove_punctuation(x))
print(mal_words_ns)

mal_words_ns.index = np.arange(len(mal_words_ns))

corpus_mal_words = []

for i in (range(len(mal_words_ns))):
  review = mal_words_ns[i]
  review = review.lower()
  review = review.split()
  
  if len(review) > 0:
      corpus_mal_words.append(review)
corpus_mal_words

print(len(corpus_mal_words))

string.punctuation

import string
string.punctuation
def remove_punctuation(text):
    no_punct = [word for word in text if not word.isalpha()]
    no_punct = [word for word in no_punct if word!= '.']
    words_wo_punct = ''.join(no_punct)
   
    return words_wo_punct
mal_words_s = df_safe['req/body/note/desc'].apply(lambda x: remove_punctuation(x))
print(mal_words_s)

import plotly.express as px
px.histogram(df_new, x = 'req/body/note/title', opacity = 0.5)

import plotly.express as px
px.histogram(df_not_safe, x = 'req/body/note/title', opacity = 0.5)

import plotly.express as px
px.histogram(df_safe, x = 'req/body/note/title', opacity = 0.5)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df_new['isSafe'] = le.fit_transform(df_new['isSafe'])

df_new['desc length'] = df_new['req/body/note/desc'].astype(str).apply(len)

y = df_new['isSafe']
X = df_new.drop(columns = 'isSafe')

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

X.index = np.arange(len(X))
corpus = []
from tqdm import tqdm
for i in tqdm(range(len(X))):
  review = X['req/body/note/desc'][i]
  review = review.lower()
  review = review.split()
  ps = PorterStemmer()
  all_stopwords = stopwords.words('english')
  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]
  review = ' '.join(review)
  corpus.append(review)

corpus

from sklearn.feature_extraction.text import CountVectorizer as CV
cv  = CV(max_features = 100,ngram_range=(1,1))

X_cv = cv.fit_transform(corpus).toarray()
y = y.values
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_cv, y, test_size = 0.20, random_state = 0)
from sklearn.naive_bayes import BernoulliNB
classifier = BernoulliNB()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)
from sklearn.metrics import accuracy_score
from sklearn import metrics
acc = accuracy_score(y_test, y_pred)
print("Accuracy of the classifier: ",acc)
print("Confusion matrix is :\n",metrics.confusion_matrix(y_test,y_pred))
print("Classification report: \n" ,metrics.classification_report(y_test,y_pred))

acc

"""An accuracy score of 84%.

Term Frequency - Inverse Document Frequency
"""

from sklearn.feature_extraction.text import TfidfVectorizer as TV
tv  = TV(ngram_range =(1,1),max_features = 3000)
X_tv = tv.fit_transform(corpus).toarray()
X_train, X_test, y_train, y_test = train_test_split(X_tv, y, test_size = 0.20, random_state = 0)
from sklearn.naive_bayes import MultinomialNB
classifier = MultinomialNB()
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
acc = accuracy_score(y_test, y_pred)
acc

"""Same accuracy as that of Bag of Words Technique, 84%.

**Deep Learning Model**
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dropout

tokenizer = Tokenizer(num_words = 100)
tokenizer.fit_on_texts(corpus)
sequences = tokenizer.texts_to_sequences(corpus)
padded = pad_sequences(sequences, padding='post')
word_index = tokenizer.word_index
vocab_size = len(tokenizer.word_index) + 1

embedding_dim = 64
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(100, embedding_dim),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(6, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.summary()

num_epochs = 10

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
m = model.fit(padded,y,epochs= num_epochs,validation_split=0.2)

"""This deep learning model has a test accuracy of 84%."""

plt.plot(m.history['accuracy'])
plt.plot(m.history['val_accuracy'])
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""**Conclusion**

The best accuracy achieved is 84.5%.

The main feature over which the safety is measured is the description feature. The concept of Natural Language Processing has been applied in order to analyse this feature, and create the learning model.

All the three models, Bag of Words model, TF-IDF model have a test accuracy of 84% and Deep Learning model has test accuracy of 84.5% .
"""

